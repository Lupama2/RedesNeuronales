\documentclass[aps,prb,twocolumn,superscriptaddress,floatfix,longbibliography]{revtex4-2}

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{wrapfig} 
\usepackage[export]{adjustbox}

\usepackage{amsmath,amssymb} % math symbols
\usepackage{bm} % bold math font
\usepackage{graphicx} % for figures
\usepackage{comment} % allows block comments
\usepackage{textcomp} % This package is just to give the text quote '
\usepackage{listings} %para agregar código

%\usepackage{ulem} % allows strikeout text, e.g. \sout{text}

\usepackage[spanish]{babel}

\usepackage{enumitem}
\setlist{noitemsep,leftmargin=*,topsep=0pt,parsep=0pt}

\usepackage{xcolor} % \textcolor{red}{text} will be red for notes
\definecolor{lightgray}{gray}{0.6}
\definecolor{medgray}{gray}{0.4}

\usepackage{hyperref}
\hypersetup{
colorlinks=true,
urlcolor= blue,
citecolor=blue,
linkcolor= blue,
bookmarks=true,
bookmarksopen=false,
}

% Code to add paragraph numbers and titles
\newif\ifptitle
\newif\ifpnumber
\newcounter{para}
\newcommand\ptitle[1]{\par\refstepcounter{para}
{\ifpnumber{\noindent\textcolor{lightgray}{\textbf{\thepara}}\indent}\fi}
{\ifptitle{\textbf{[{#1}]}}\fi}}
%\ptitletrue  % comment this line to hide paragraph titles
%\pnumbertrue  % comment this line to hide paragraph numbers

% minimum font size for figures
\newcommand{\minfont}{6}

% Uncomment this line if you prefer your vectors to appear as bold letters.
% By default they will appear with arrows over them.
% \renewcommand{\vec}[1]{\bm{#1}}

%Cambiar Cuadros por Tablas y lista de...
%\renewcommand{\listtablename}{Índice de tablas}
\renewcommand{\tablename}{Tabla}
\renewcommand{\date}{Fecha}

% \graphicspath{ {C:/Users/lupam/Mi unidad/Pablo Chehade/Instituto Balseiro (IB)/Laboratorio Avanzado/Informe/V5/Figures} } %Para importar imagenes desde una carpeta


\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  keywordstyle=\color{blue},
  commentstyle=\color{green},
  stringstyle=\color{red},
} %Configuración para el bloque de código


\usepackage[bottom]{footmisc} %para que las notas al pie aparezcan en la misma página



\begin{comment}

%Comandos de interes:

* Para ordenar el documento:
\section{Introducción}
\section{\label{sec:Formatting}Formatting} %label para luego hacer referencia a esa sección

\ptitle{Start writing while you experiment} %pone nombre y título al documento dependiendo de si en el header están los comandos \ptitletrue y \pnumbertrue

* Ecuaciones:
\begin{equation}
a^2+b^2=c^2 \,.
\label{eqn:Pythagoras}
\end{equation}

* Conjunto de ecuaciones:
\begin{eqnarray}
\label{eqn:diagonal}
\nonumber d & = & \sqrt{a^2 + b^2 + c^2} \\
& = & \sqrt{3^2+4^2+12^2} = 13
\end{eqnarray}

* Para hacer items / enumerar:
\begin{enumerate}
  \item
\end{enumerate}

\begin{itemize}
  \item
\end{itemize}

* Figuras:
\begin{figure}[h]
    \includegraphics[clip=true,width=\columnwidth]{pixel-compare}
    \caption{}
     \label{fig:pixels}
\end{figure}

* Conjunto de figuras:
(no recuerdo)


* Para hacer referencias a fórmulas, tablas, secciones, ... dentro del documento:
\ref{tab:spacing}

* Para citar
Elementos de .bib
\cite{WhitesidesAdvMat2004}
url
\url{http://www.mendeley.com/}\\

* Agradecimientos:
\begin{acknowledgments}
We acknowledge advice from Jessie Zhang and Harry Pirie to produce Fig.\ \ref{fig:pixels}.
\end{acknowledgments}

* Apéndice:
\appendix
\section{\label{app:Mendeley}Mendeley}

* Bibliografía:
\bibliography{Hoffman-example-paper}

\end{comment}


\begin{document}

% Allows to rewrite the same title in the supplement
\newcommand{\mytitle}{Aprendizaje no supervisado}

\title{\mytitle}

\author{Pablo Chehade \\
    \small \textit{pablo.chehade@ib.edu.ar} \\
    \small \textit{Redes Neuronales, Instituto Balseiro, CNEA-UNCuyo, Bariloche, Argentina, 2023} \\}
    
\maketitle




\section*{Ejercicio 1}


Se entrenó de manera no supervisada una red neuronal lineal de una sola capa con cuatro entradas y una salida. Los datos de entrada presentan una distribución gaussiana con matriz de correlación $\Sigma$



\[
\Sigma = \begin{bmatrix}
2 & 1 & 1 & 1 \\
1 & 2 & 1 & 1 \\
1 & 1 & 2 & 1 \\
1 & 1 & 1 & 2 \\
\end{bmatrix}
.
\]
El autovector asociado al mayor autovalor de $\Sigma$ es $\vec{v} = \frac{1}{2}(1, 1, 1, 1)$. 


Se partió de un vector de pesos $\vec{w} = (w_1, w_2, w_3, w_4)$, con cada componente dada por una distribución uniforme con máximo $0.01$. Luego, se aplicó la regla de Oja
\[
\Delta w_j = \eta V ( \xi_j - V w_j ).
\]
Aquí, $\eta$ representa la tasa de aprendizaje, $V$ es la salida de la red y $\xi_j$ es la componente $j$ del dato de entrada $\vec{\xi}$. En base a la teoría desarrollada para la regla de Oja, se espera que el vector de pesos $\vec{w}$ tienda al autovector $\vec{v}$.


En la figura \ref{fig:ej1_fig1}, se ilustra la evolución del módulo de los pesos $|w_j|$, las modificaciones $\Delta w_j$ y la diferencia entre $w_j$ y $v_j$, para $\eta = 0.001$. Tras un período transitorio, se observa que las componentes de $\vec{w}$ tienden hacia el valor $0.5$, indicando convergencia hacia $\vec{v}$. Dependiendo de las condiciones iniciales, el vector $\vec{w}$ tiende a alinearse en la dirección $+\vec{v}$ o $-\vec{v}$. Debido a esto se graficó el módulo de cada componente. Posteriormente, en estado estacionario, los pesos continúan modificándose. Por ello, en la figura \ref{fig:ej1_fig2}, se procesan los datos anteriores realizando un promedio en cada paso de tiempo de los 200 pasos adyacentes. Esto resulta en un comportamiento más suave y una reducción notable en las variaciones de $\Delta w_j$ de hasta un orden de magnitud. Luego de este procesamiento de los datos se confirma nuevamente que $\vec{w}$ tiende a $\vec{v}$.

Por último, en la figura \ref{fig:ej1_fig3}, se evolucionó el sistema con $\eta = 0.01$. El sistema muestra una convergencia más rápida y variaciones $\Delta w_j$ de mayor magnitud. Además, se determinó que existe un valor superior límite para $\eta$, más allá del cual el sistema tiende a diverger.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ej1_fig1.png}
        \caption{\label{fig:ej1_fig1}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ej1_fig2.png}
        \caption{\label{fig:ej1_fig2}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ej1_fig3.png}
        \caption{\label{fig:ej1_fig3}}
    \end{subfigure}
       \caption{Evolución del módulo de los pesos $w_j$, las modificaciones $\Delta w_j$ y la diferencia entre $w_j$ y $v_j$ durante el entrenamiento de la red neuronal. En \ref{sub@fig:ej1_fig1} se empleó una tasa de aprendizaje de $\eta = 0.001$. En \ref{sub@fig:ej1_fig2} se empleó la misma tasa de aprendizaje pero además se procesaron los datos promediando en cada paso de tiempo sobre los 200 pasos adyacentes. En \ref{sub@fig:ej1_fig3} se empleó una tasa de aprendizaje de $\eta = 0.01$.}
       \label{fig:three graphs}
  \end{figure}

\section*{Ejercicio 2}

Se entrenó una red neuronal de Kohonen con dos neuronas de entrada y diez neuronas de salida alineadas en una línea. La red se alimenta mediante una distribución de entrada definida como:

\[ P(\xi) = P(r, \theta) = 
  \begin{cases} 
   \text{constante} & \text{si } r \in [0.9, 1.1], \, \theta \in [0, \pi] \\
   0 & \text{si no},
  \end{cases} \]
donde $r$ y $\theta$ son las coordenadas polares del vector $\vec{\xi}$.

Los pesos iniciales \(w_{ij}\), donde \(i\) representa la neurona de salida y \(j\) la dimensión del espacio de entrada, se asignan aleatoriamente con una distribución uniforme con un valor máximo de 0.1. En cada iteración del entrenamiento, se selecciona un vector de entrada \(\vec{\xi}\), y se identifica la neurona "ganadora" \(i^*\) como aquella que minimiza la distancia euclidiana con el vector de entrada, es decir

\[ i^* = \text{argmin}_i |\vec{w}_i - \vec{\xi}|, \]
donde $\vec{w}_i = (w_{i1}, w_{i2})$. Posteriormente, se actualizan los pesos de todas las neuronas utilizando la regla
\[ \Delta \vec{w}_i = \eta \, \Gamma(i, i^*) \, (\vec{\xi} - \vec{w}_i), \]
donde 
\[ \Gamma(i, i^*) = \exp\left( - \frac{(i - i^*)^2}{2 \sigma^2} \right) \]
es la función de vecindad y \(\eta\) es la tasa de aprendizaje.

Se realizó un análisis sobre la evolución de los pesos de la red variando los parámetros \(\eta\) y \(\sigma\), y se observó cómo afectan la convergencia, haciendo incapié cómo se modifican los pesos durante el entrenamiento.

Se observaron diferentes comportamientos al variar los parámetros \(\eta\) y \(\sigma\). En todos los casos se estudió la convergencia de los pesos $\vec{w}_i$ mediante su evolución graficada en el espacio de los datos de entrada (figura \ref{fig:ej2_fig1})

\begin{enumerate}
    \item \(\eta = 0.1\) y \(\sigma = 1\): Inicialmente los pesos \(\vec{w}_i\) son cercanos entre sí. Luego se desplazan hacia las regiones donde \(P(\vec{\xi})\) es no nula. Finalmente, los pesos se organizan sobre la región, organizados de acuerdo a la disposición de las neuronas sobre la línea.

    \item \(\eta = 0.1\) y \(\sigma = 2\): En este caso, los pesos se mueven más juntos en comparación con el caso anterior debido a un mayor valor de \(\sigma\), que incrementa el efecto de la función de vecindad. Esto se puede entender más claramente en el caso extremo en el que $\sigma \rightarrow \infty$. En tal caso, $\Gamma \rightarrow 1$ y todas las neuronas se actualizan en la misma magnitud hacia la posición del dato de entrada $\vec{\xi}$. En cuanto al estado final, incluso con una gran cantidad de pasos de entrenamiento no se llega al mismo estado que antes. Una forma de resolver este problema podría ser aprovechando el comportamiento visto en el caso anterior y disminuir progresivamente $\sigma$ a medida que se realiza el entrenamiento.
    \item \(\eta = 0.01\) y \(\sigma = 1\): Con una tasa de aprendizaje reducida, la evolución es más lenta. Además, los pesos muestran variaciones menores durante el entrenamiento. A pesar de la lentitud, con suficientes iteraciones el sistema converge a la misma condición que el primer caso.
\end{enumerate}
El estudio sugiere que la elección de \(\eta\) y \(\sigma\) es crucial. Un \(\eta\) más bajo lleva a una convergencia más lenta pero con menor ruido, mientras que un \(\sigma\) más alto promueve una adaptación global pero menos detallada de los pesos. 



\onecolumngrid


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig1_1.png}
      \caption{\label{fig:ej2_fig1_1}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig1_2.png}
      \caption{\label{fig:ej2_fig1_2}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig1_3.png}
      \caption{\label{fig:ej2_fig1_3}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig1_4.png}
      \caption{\label{fig:ej2_fig1_4}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig2_1.png}
      \caption{\label{fig:ej2_fig2_1}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig2_2.png}
      \caption{\label{fig:ej2_fig2_2}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig2_3.png}
      \caption{\label{fig:ej2_fig2_3}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig2_4.png}
      \caption{\label{fig:ej2_fig2_4}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig3_1.png}
      \caption{\label{fig:ej2_fig3_1}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig3_2.png}
      \caption{\label{fig:ej2_fig3_2}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig3_3.png}
      \caption{\label{fig:ej2_fig3_3}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ej2_fig3_4.png}
      \caption{\label{fig:ej2_fig3_4}}
  \end{subfigure}
     \caption{Evolución de los peso $\vec{w}_i$ con $i = 1$, 2, 3, $\ddots$, 10 en el espacio de los datos de entrada para distintos pasos de entrenamiento $\mathrm{N_{train}}$. Se empleó \ref{sub@fig:ej2_fig1_1}-\ref{sub@fig:ej2_fig1_4} $\eta = 0.1$, $\sigma = 1$, \ref{sub@fig:ej2_fig2_1}-\ref{sub@fig:ej2_fig2_4} $\eta = 0.1$, $\sigma = 2$, \ref{sub@fig:ej2_fig3_1}-\ref{sub@fig:ej2_fig3_4} $\eta = 0.01$, $\sigma = 1$. La región semicircular corresponde a la región donde se encuentran los datos de entrada. En negro se grafica la posición de los pesos finales para cada valor de $\mathrm{N_{train}}$, junto al número correspondiente de neurona. La numeración de las neuronas se realiza en base a que se encuentran dispuestas en una línea. En color se grafican las posiciones previas. No se grafican los ejes por simplicidad.}
     \label{fig:ej2_fig1}
\end{figure}


\newpage

\section*{Apéndice}
A continuación se desarrolla el código empleado durante este trabajo implementado en Python.




\begin{lstlisting}[language=Python]
    
    ### Ejercicio 1
    #Import libraries
    import numpy as np
    import matplotlib
    import matplotlib.pyplot as plt
    
    #Parameters
    w_ini_max = 0.01 #valor maximo del peso inicial

    #Def correlation matrix
    C = np.array([[2, 1, 1, 1],
                  [1, 2, 1, 1],
                  [1, 1, 2, 1],
                  [1, 1, 1, 2]])
    
    C_12 = np.array([[1.309, 0.309, 0.309, 0.309],
                    [0.309, 1.309, 0.309, 0.309],
                    [0.309, 0.309, 1.309, 0.309],
                    [0.309, 0.309, 0.309, 1.309]])
    
    #Calculo los autovalores y autovectores de C
    eigvals, eigvecs = np.linalg.eig(C)
    #Calculo el autovector con el mayor autovalor
    eigvec_max = eigvecs[:, np.argmax(eigvals)]
    #Imprimo
    print('El autovector con el mayor autovalor es: ', eigvec_max)
    
    def ejemplo():
        #Genero 4 nros con distribucion normal
        z = np.random.randn(4)
        #Multiplico por C_12
        x = np.dot(C_12, z)
        return x
    
    def inicializacion_pesos():
        #Genero 4 nros con distribucion uniforme
        w = np.random.uniform(-w_ini_max, w_ini_max, 4)
        return w
        
    
    def actualizacion_pesos(w, eta):
        #Genero un ejemplo
        xi = ejemplo()
        #Calculo la salida
        V = np.dot(w, xi)
        #Calculo delta_w
        delta_w = eta*V*(xi - V*w)
        return w + delta_w
    
    def train(N_train, eta):
        #Genero la matriz de pesos
        w_matrix = np.zeros([N_train, 4])
        #Genero los pesos iniciales
        w = inicializacion_pesos()
        w_matrix[0] = w
        #Entreno N_train pasos
        for i in range(1,N_train):
            w = actualizacion_pesos(w, eta)
            w_matrix[i] = w
        return w_matrix
    
    #Entreno una red
    N_train = 5000
    
    #Promedio w_matrix y delta_w para cada iindice en una caja de ancho M con np.convolve
    def plt_w(M, eta):
        w_matrix = train(N_train, eta)
        #Calculo los delta_w
        delta_w = w_matrix[1:] - w_matrix[:-1]
    
        w_matrix_mean = np.empty([N_train - M + 1, 4])
        delta_w_mean = np.empty([N_train - M, 4])
        for i in range(4):
            w_matrix_mean[:, i] = np.convolve(w_matrix[:, i], np.ones(M)/M, mode='valid')
            delta_w_mean[:, i] = np.convolve(delta_w[:, i], np.ones(M)/M, mode='valid')
    
        # Grafico los pasos en el tiempo y en otra grafica, los cambios delta w_j entre pasos
        fig, ax = plt.subplots(3, 1, figsize=(9.5, 5.5), sharex=True)
        fig.subplots_adjust(hspace=0.1)
    
        ax[0].plot(np.abs(w_matrix_mean))
        # ax[0].set_xlabel('Pasos')
        ax[0].set_ylabel('$|w_j|$')
        ax[0].labels = ['1', '2', '3', '4']
        #Grafico las labels arriba fuera del grafico
        ax[0].legend(ax[0].labels, loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=4)
        ax[0].set_ylim(0, 1)
        # ax[0].set_xscale("log")
        ax[0].grid()
    
        ax[1].plot(delta_w_mean)
        # ax[1].set_xlabel('Pasos')
        ax[1].set_ylabel('$\Delta w_j$')
        ax[1].labels = ['1', '2', '3', '4']
        # ax[1].legend(ax[1].labels)
        ax[1].grid()
    
        ax[2].plot(np.abs(np.abs(w_matrix_mean) - np.abs(eigvec_max)))
        ax[2].set_xlabel('Pasos')
        ax[2].set_ylabel('Error')
        ax[2].labels = ['1', '2', '3', '4']
        #Grafico las labels arriba fuera del grafico
        # ax[2].legend(ax[2].labels)
        ax[2].set_yscale("log")
        ax[2].grid()
    
        plt.show()
    
    #Hago graficos para el informe
    plt_w(1, 0.001)
    plt.savefig('Informe/ej1_fig1.png', bbox_inches='tight', dpi=300)
    
    plt_w(200, 0.001)
    plt.savefig('Informe/ej1_fig2.png', bbox_inches='tight', dpi=300)
    
    plt_w(1, 0.01)
    plt.savefig('Informe/ej1_fig3.png', bbox_inches='tight', dpi=300)
    
    # ## Ejercicio 2
    #Def parametros globales
    r1 = 0.9
    r2 = 1.1
    N = 2
    M = 10
    w_ini_max = 0.1
    
    #Def seed
    np.random.seed(0)
    
    #Genero pares de nros aleatorios (x,y) con distribucion uniforme sobre un circulo
    def generar_datos(n):
        r_array = np.empty([n,2])
        contador = 0
        while(contador < n):
            #Genero nros aleatorios con distribucion uniforme entre 0 y 1
            x = (np.random.rand()*2 - 1)*r2
            y = (np.random.rand()*2 - 1)*r2
            #Me fijo si pertenece al anillo
            r = np.sqrt(x**2 + y**2)
            theta = np.arctan2(y,x)
            if r1 < r < r2 and 0 < theta < np.pi:
                r_array[contador] = np.array([x,y])
                contador += 1
        return r_array
    
    def Vecindad(i,i_star,sigma):
        return np.exp(-((i-i_star)**2)/(2*sigma**2))
    
    def algoritmo_de_Kohonen(w, x, sigma, eta):
        #Calculo la neurona ganadora
        i_star = np.argmin(np.linalg.norm(w-x,axis=1))
        #Actualizo los pesos
        delta_w = eta*Vecindad(np.arange(M),i_star,sigma).reshape(M,1)*(x-w)
        w += delta_w
        return w
    
    def entrenamiento_Kohonen(N_train, sigma, eta):
        #Inicializo pesos de la red
        w = np.random.rand(M,N)*w_ini_max
        w_matriz = np.empty([N_train,M,N])
        #Genero los datos
        r_array = generar_datos(N_train)
        #Entreno la red
        for i in range(N_train):
            w = algoritmo_de_Kohonen(w, r_array[i], sigma, eta)
            #Guardo el valor
            w_matriz[i] = w
        return w_matriz
    
    def plt_entrenamiento_Kohonen(N_train, sigma, eta):
        #Redef seed
        np.random.seed(0)
        w_matriz = entrenamiento_Kohonen(N_train, sigma, eta)
    
        #Graph los pesos en el tiempo
        fig, ax = plt.subplots(figsize = (5,4))
        color_vec = ["tab:red", "tab:blue", "tab:green", "tab:orange", "tab:purple", "tab:brown", "tab:pink", "tab:gray", "tab:olive", "tab:cyan"]
    
        #Grafico los pesos
        for i in range(M):
            ax.plot(w_matriz[0:,i,0],w_matriz[0:,i,1],'o-',color=color_vec[i], alpha = 0.1)
        for i in range(M):
            #Pinto de otro color el ultimo punto
            ax.plot(w_matriz[-1,i,0],w_matriz[-1,i,1],'o-',color="k", alpha = 1)
            ax.text(w_matriz[-1,i,0],w_matriz[-1,i,1],str(i),color="k",alpha=1)
    
        #Grafico un anillo entre r1 y r2 con theta entre 0 y pi
        theta = np.linspace(0,np.pi,100)
        ax.plot(r1*np.cos(theta),r1*np.sin(theta),'k')
        ax.plot(r2*np.cos(theta),r2*np.sin(theta),'k')
        ax.plot([-r2,-r1],[0,0],'k'); ax.plot([r1,r2],[0,0],'k')
        ax.set_xlim(-1.2,1.2)
        ax.set_ylim(-0.1,1.2)
    
        #Quito los tickslabels
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.set_xticks([])
        ax.set_yticks([])
        #Agrego arriba a la izquierda el valor de N_train
        ax.text(-1.1,1.05,'$\mathrm{N_{train}}$ = '+str(N_train),color="k",alpha=1)
    
        plt.show()
        return
    
    #Guardo graficos particulares y los guardo
    # plt_entrenamiento_Kohonen(N_train, sigma, eta)
    
    #Figura 1
    sigma = 1
    eta = 0.1
    
    plt_entrenamiento_Kohonen(10, sigma, eta)
    plt.savefig('Informe/ej2_fig1_1.png', bbox_inches='tight', dpi=300)
    
    #Descomentar las siguientes lineas para hacer la variacion de N_train, eta y sigma
    # plt_entrenamiento_Kohonen(200, sigma, eta)
    # plt.savefig('Informe/ej2_fig1_2.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(750, sigma, eta)
    # plt.savefig('Informe/ej2_fig1_3.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(1750, sigma, eta)
    # plt.savefig('Informe/ej2_fig1_4.png', bbox_inches='tight', dpi=300)
    
    # #Figura 2
    # sigma = 2
    # eta = 0.1
    
    # plt_entrenamiento_Kohonen(10, sigma, eta)
    # plt.savefig('Informe/ej2_fig2_1.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(200, sigma, eta)
    # plt.savefig('Informe/ej2_fig2_2.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(750, sigma, eta)
    # plt.savefig('Informe/ej2_fig2_3.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(50000, sigma, eta)
    # plt.savefig('Informe/ej2_fig2_4.png', bbox_inches='tight', dpi=300)
    
    # #Figura 3
    # sigma = 1
    # eta = 0.01
    
    # plt_entrenamiento_Kohonen(10, sigma, eta)
    # plt.savefig('Informe/ej2_fig3_1.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(200, sigma, eta)
    # plt.savefig('Informe/ej2_fig3_2.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(750, sigma, eta)
    # plt.savefig('Informe/ej2_fig3_3.png', bbox_inches='tight', dpi=300)
    
    # plt_entrenamiento_Kohonen(6000, sigma, eta)
    # plt.savefig('Informe/ej2_fig3_4.png', bbox_inches='tight', dpi=300)
        

\end{lstlisting}

\bibliography{Chehade_practica_5.bib}

\end{document}





